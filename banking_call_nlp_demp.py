# -*- coding: utf-8 -*-
"""Banking_Call_NLP_Demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/183MadrECfto2l4c6B8Jp63iEw0S19q0R

# NLP-based Topic Modeling & Intent Discovery Demo
End-to-end pipeline on banking call transcripts: preprocessing → topic modeling → intent clustering → summarization → retrieval QA.
**Files included:** `banking_calls_sample.csv`
"""

# Optional: install dependencies (uncomment if needed)
# !pip install -q pandas scikit-learn sentence-transformers hdbscan umap-learn transformers torch matplotlib

import pandas as pd
import numpy as np
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Robust imports for optional libs
try:
    from sentence_transformers import SentenceTransformer
    _HAS_ST = True
except Exception as e:
    print("sentence-transformers not available, falling back to TF-IDF only:", e)
    _HAS_ST = False

try:
    import hdbscan
    _HAS_HDBSCAN = True
except Exception as e:
    print("hdbscan not available, will use KMeans:", e)
    _HAS_HDBSCAN = False

try:
    from transformers import pipeline
    _HAS_HF = True
except Exception as e:
    print("transformers not available; summarization will be simple extractive:", e)
    _HAS_HF = False

print("Libraries ready.")

# Load sample data
df = pd.read_csv(r"/mnt/data/banking_calls_sample.csv")
df.head()

# Basic cleaning
def clean_text(t):
    t = str(t).lower()
    t = re.sub(r"\s+", " ", t)
    t = re.sub(r"[^a-z0-9 \-']", " ", t)
    return t.strip()

df['cleaned'] = df['transcript'].apply(clean_text)
df[['call_id', 'transcript', 'cleaned']].head()

# TF-IDF representation
tfidf = TfidfVectorizer(stop_words='english', max_features=3000, ngram_range=(1,2))
X = tfidf.fit_transform(df['cleaned'])
len(tfidf.get_feature_names_out()), X.shape

"""## Topic Modeling (LDA & NMF)"""

# LDA topics
lda = LatentDirichletAllocation(n_components=6, random_state=42, learning_method='batch')
lda_topics = lda.fit_transform(X)

terms = tfidf.get_feature_names_out()
def top_terms_for_components(components, topn=10):
    topics = []
    for comp in components:
        top_idx = np.argsort(comp)[-topn:][::-1]
        topics.append([terms[i] for i in top_idx])
    return topics

lda_top_terms = top_terms_for_components(lda.components_, topn=12)
for i, t in enumerate(lda_top_terms):
    print(f"LDA Topic {i}: ", ", ".join(t))

# NMF topics (often more coherent with TF-IDF)
nmf = NMF(n_components=6, random_state=42, init='nndsvd')
nmf_topics = nmf.fit_transform(X)
nmf_top_terms = top_terms_for_components(nmf.components_, topn=12)
for i, t in enumerate(nmf_top_terms):
    print(f"NMF Topic {i}: ", ", ".join(t))

"""## Intent Discovery (Embeddings + Clustering)"""

# Build embeddings
if _HAS_ST:
    st_model = SentenceTransformer('all-MiniLM-L6-v2')
    E = st_model.encode(df['cleaned'], show_progress_bar=False)
else:
    # Fallback to TF-IDF dense vectors
    E = X.toarray()

import numpy as np
print("Embedding shape:", np.array(E).shape)

# Cluster with HDBSCAN if available, else KMeans
if _HAS_HDBSCAN:
    clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=2)
    labels = clusterer.fit_predict(E)
    algo = "HDBSCAN"
else:
    k = 6
    km = KMeans(n_clusters=k, random_state=42, n_init='auto')
    labels = km.fit_predict(E)
    algo = "KMeans"

import pandas as pd
df['intent_cluster'] = labels
print("Clustering algorithm:", algo)
df['intent_cluster'].value_counts().sort_index()

# Simple cluster summaries: top TF-IDF terms per cluster
from collections import defaultdict
cluster_texts = defaultdict(list)
for text, lab in zip(df['cleaned'], df['intent_cluster']):
    cluster_texts[lab].append(text)

def top_tfidf_terms_for_docs(docs, vectorizer, topn=10):
    if not docs:
        return []
    X_local = vectorizer.transform(docs)
    mean_vec = np.asarray(X_local.mean(axis=0)).ravel()
    idx = np.argsort(mean_vec)[-topn:][::-1]
    terms = vectorizer.get_feature_names_out()
    return [terms[i] for i in idx]

cluster_keywords = {c: top_tfidf_terms_for_docs(txts, tfidf, topn=10)
                    for c, txts in cluster_texts.items() if c != -1}
cluster_keywords

# Silhouette score (only if more than 1 cluster label present)
from sklearn.metrics import silhouette_score
import numpy as np
unique_labels = [l for l in np.unique(df['intent_cluster']) if l != -1]
if len(unique_labels) > 1:
    try:
        sil = silhouette_score(np.array(E), [l if l != -1 else 9999 for l in df['intent_cluster']], metric='euclidean')
        print("Silhouette score (approx, treating -1 as separate):", sil)
    except Exception as e:
        print("Silhouette score could not be computed:", e)
else:
    print("Not enough clusters for silhouette scoring.")

"""## Visualization (optional UMAP)"""

# Optional 2D projection via UMAP for scatter plot
try:
    import umap
    reducer = umap.UMAP(random_state=42, n_neighbors=10, min_dist=0.1)
    Z = reducer.fit_transform(E)
    import matplotlib.pyplot as plt
    plt.figure(figsize=(6,5))
    plt.scatter(Z[:,0], Z[:,1], c=df['intent_cluster'], s=40)
    plt.title("UMAP projection colored by cluster")
    plt.xlabel("UMAP-1"); plt.ylabel("UMAP-2")
    plt.show()
except Exception as e:
    print("UMAP not available:", e)

"""## Abstractive Summarization of a Cluster"""

# Summarize the most populous cluster
target_cluster = df['intent_cluster'].value_counts().index[0]
subset = df[df['intent_cluster']==target_cluster]['transcript'].tolist()[:6]
cluster_text = " ".join(subset)

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
except Exception as e:
    pass

if _HAS_HF:
    try:
        from transformers import pipeline
        summarizer = pipeline("summarization", model="t5-small")
        out = summarizer(cluster_text, max_length=80, min_length=25, do_sample=False)
        print("Summary:", out[0]['summary_text'])
    except Exception as e:
        print("HF summarization failed; falling back to extractive:", e)
        # Simple extractive: pick 2 most central sentences by TF-IDF cosine
        sents = subset
        V = TfidfVectorizer(stop_words='english').fit_transform(sents)
        sim = cosine_similarity(V)
        centrality = sim.mean(axis=1)
        top_idx = centrality.argsort()[-2:][::-1]
        print("Summary:", " ".join([sents[i] for i in top_idx]))
else:
    # Simple extractive fallback
    sents = subset
    V = TfidfVectorizer(stop_words='english').fit_transform(sents)
    sim = cosine_similarity(V)
    centrality = sim.mean(axis=1)
    top_idx = centrality.argsort()[-2:][::-1]
    print("Summary:", " ".join([sents[i] for i in top_idx]))

"""## Retrieval-style QA (without full RAG)"""

# Given a business question, retrieve top similar calls and summarize
question = "How to dispute an unauthorized credit card transaction and get refund?"
q_vec = tfidf.transform([question])
from sklearn.metrics.pairwise import cosine_similarity
sims = cosine_similarity(q_vec, X).ravel()
top_k = sims.argsort()[-5:][::-1]
print("Top related call IDs:", df.iloc[top_k]['call_id'].tolist())
print("Snippets:")
for i in top_k:
    print("-", df.iloc[i]['transcript'])

# Summarize those snippets
snips = df.iloc[top_k]['transcript'].tolist()
if _HAS_HF:
    try:
        from transformers import pipeline
        summarizer = pipeline("summarization", model="t5-small")
        out = summarizer(" ".join(snips), max_length=80, min_length=25, do_sample=False)
        print("\nAnswer Summary:", out[0]['summary_text'])
    except Exception as e:
        print("HF summarization failed; returning extractive:", e)
        print("\nAnswer Summary:", " ".join(snips[:2]))
else:
    print("\nAnswer Summary:", " ".join(snips[:2]))

"""---
**Done.** You can swap models, tweak `n_components`, or export results to CSV for the report.
"""
